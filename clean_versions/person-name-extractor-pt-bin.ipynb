{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"markdown","source":"### Installing external libraries","metadata":{}},{"cell_type":"code","source":"!pip install datasets evaluate transformers[sentencepiece]\n!apt install git-lfs\n# for using huggingface in kaggle\n! pip install -U git+https://github.com/huggingface/transformers.git\n! pip install -U git+https://github.com/huggingface/accelerate.git\n# for evaluation\n!pip install seqeval","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:13:50.459087Z","iopub.execute_input":"2023-07-11T14:13:50.459767Z","iopub.status.idle":"2023-07-11T14:15:30.263412Z","shell.execute_reply.started":"2023-07-11T14:13:50.459732Z","shell.execute_reply":"2023-07-11T14:15:30.262207Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nCollecting evaluate\n  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: transformers[sentencepiece] in /opt/conda/lib/python3.10/site-packages (4.30.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.23.5)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (1.5.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.65.0)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.4)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.16.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (2023.6.3)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.3.1)\nRequirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.1.99)\nRequirement already satisfied: protobuf<=3.20.3 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (3.20.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.5.7)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\nInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.0\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\ngit-lfs is already the newest version (3.0.2-1ubuntu0.2).\n0 upgraded, 0 newly installed, 0 to remove and 13 not upgraded.\nCollecting git+https://github.com/huggingface/transformers.git\n  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-vzklf9jo\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-vzklf9jo\n  Resolved https://github.com/huggingface/transformers.git to commit 2489e380e45177eb3da108366f65b8108b2815c5\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (0.3.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (4.65.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0.dev0) (2023.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0.dev0) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.31.0.dev0) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0.dev0) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0.dev0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0.dev0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0.dev0) (2023.5.7)\nBuilding wheels for collected packages: transformers\n  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for transformers: filename=transformers-4.31.0.dev0-py3-none-any.whl size=7344228 sha256=fc90f1d45457845d424a9f10bd6a424b13cee96e64b631c47652218677f3ff00\n  Stored in directory: /tmp/pip-ephem-wheel-cache-kl6ed24x/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\nSuccessfully built transformers\nInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.30.2\n    Uninstalling transformers-4.30.2:\n      Successfully uninstalled transformers-4.30.2\nSuccessfully installed transformers-4.31.0.dev0\nCollecting git+https://github.com/huggingface/accelerate.git\n  Cloning https://github.com/huggingface/accelerate.git to /tmp/pip-req-build-vulh_ed4\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/accelerate.git /tmp/pip-req-build-vulh_ed4\n  Resolved https://github.com/huggingface/accelerate.git to commit 64d7b58c4415a5714c25caa5acb428bec26f94e1\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0.dev0) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0.dev0) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0.dev0) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0.dev0) (6.0)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0.dev0) (2.0.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate==0.21.0.dev0) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0.dev0) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0.dev0) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0.dev0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0.dev0) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0.dev0) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.21.0.dev0) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.21.0.dev0) (1.3.0)\nBuilding wheels for collected packages: accelerate\n  Building wheel for accelerate (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for accelerate: filename=accelerate-0.21.0.dev0-py3-none-any.whl size=241954 sha256=732ac79b81c41ea959569b46883ce0dbd8b1af5e37898ea14c44432ed73e7de7\n  Stored in directory: /tmp/pip-ephem-wheel-cache-5nlmhl11/wheels/9c/a3/1e/47368f9b6575655fe9ee1b6350cfa7d4b0befe66a35f8a8365\nSuccessfully built accelerate\nInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.20.3\n    Uninstalling accelerate-0.20.3:\n      Successfully uninstalled accelerate-0.20.3\nSuccessfully installed accelerate-0.21.0.dev0\nCollecting seqeval\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.23.5)\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.2.2)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.11.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\nBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=b31d3fb8be88b3a478db412982c4b2e35bc972bb004c4ce77b2ef42e8cc4015b\n  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\nSuccessfully built seqeval\nInstalling collected packages: seqeval\nSuccessfully installed seqeval-1.2.2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### setting up hugginface hub account for using inference (Pipeline)","metadata":{}},{"cell_type":"code","source":"!git config --global user.email \"imtiazahmed5573@gmail.com\" # put your huggingface account mail here\n!git config --global user.name \"imtiaz114\" # put your huggingface account user name here","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:15:30.266139Z","iopub.execute_input":"2023-07-11T14:15:30.266527Z","iopub.status.idle":"2023-07-11T14:15:32.182030Z","shell.execute_reply.started":"2023-07-11T14:15:30.266490Z","shell.execute_reply":"2023-07-11T14:15:32.179959Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Enter your code by tapping on the \"Hugging Face tokens\" portion of generated message\nfrom huggingface_hub import notebook_login\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:15:32.184710Z","iopub.execute_input":"2023-07-11T14:15:32.185135Z","iopub.status.idle":"2023-07-11T14:15:32.408293Z","shell.execute_reply.started":"2023-07-11T14:15:32.185096Z","shell.execute_reply":"2023-07-11T14:15:32.407202Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25ed5b8f3e584562ac199e56c2222bfb"}},"metadata":{}}]},{"cell_type":"markdown","source":"### Importing necessary libraries","metadata":{}},{"cell_type":"code","source":"# basic libraries\nimport pandas as pd\nimport numpy as np\nimport itertools\nimport gc\n\n# evaluation\nimport evaluate\n\n# dataset\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Dataset, DatasetDict\n\n# Tokenizer\nfrom transformers import AutoTokenizer\n# collat fn\nfrom transformers import DataCollatorForTokenClassification\n# model\nfrom transformers import TFAutoModelForTokenClassification\n# callback (useful for using pipeline function for inference)\nfrom transformers.keras_callbacks import PushToHubCallback\n\n# prediction\nfrom transformers import pipeline\n\n# tensorflow\nfrom transformers import create_optimizer\nimport tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:24:20.492893Z","iopub.execute_input":"2023-07-11T14:24:20.493325Z","iopub.status.idle":"2023-07-11T14:24:36.642671Z","shell.execute_reply.started":"2023-07-11T14:24:20.493293Z","shell.execute_reply":"2023-07-11T14:24:36.641666Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"# paths\nmodel_checkpoint = \"sagorsarker/mbert-bengali-ner\"\ndirname = \"/kaggle/working/mbert-ner-baseline-1\" # model-saving-directory\n\ndataset_path = \"/kaggle/input/hishab-dataset-person-extractor/hishab_ds/hishab_ds/concatenated_bangla_ner_multiclass.csv\"","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:24:36.644742Z","iopub.execute_input":"2023-07-11T14:24:36.645148Z","iopub.status.idle":"2023-07-11T14:24:36.653625Z","shell.execute_reply.started":"2023-07-11T14:24:36.645112Z","shell.execute_reply":"2023-07-11T14:24:36.652545Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Hyperparamters\nnum_epochs = 2 # 2 or 4 epochs are suitable for this task\nINIT_LR    = 2e-5\nWD         = 0.01","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:37:08.251866Z","iopub.execute_input":"2023-07-11T14:37:08.252571Z","iopub.status.idle":"2023-07-11T14:37:08.260192Z","shell.execute_reply.started":"2023-07-11T14:37:08.252533Z","shell.execute_reply":"2023-07-11T14:37:08.258806Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"markdown","source":"### Reading the dataset\n\nThe data here was prepared in another notebook named \" \", please refer that to get an idea about data preparation","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(dataset_path,index_col=False)\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:24:36.671007Z","iopub.execute_input":"2023-07-11T14:24:36.671306Z","iopub.status.idle":"2023-07-11T14:24:36.842776Z","shell.execute_reply.started":"2023-07-11T14:24:36.671281Z","shell.execute_reply":"2023-07-11T14:24:36.841833Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                              tokens  \\\n0  ['\\ufeffউন্নয়নের', 'বিস্ময়', 'বাংলাদেশ', 'বাংল...   \n1  ['অর্থনীতি', 'ও', 'আর্থসামাজিক', 'বেশির', 'ভাগ...   \n2  ['নিম্ন', 'আয়ের', 'দেশগুলোকে', 'ছাড়িয়েছে', 'তো...   \n3  ['আন্তর্জাতিক', 'মুদ্রা', 'তহবিল', '(', 'আইএমএ...   \n4  ['সবাইকে', 'অন্তর্ভুক্ত', 'করে', 'প্রবৃদ্ধি', ...   \n\n                                                tags  \n0  ['O', 'O', 'B-LOC', 'B-LOC', 'O', 'O', 'O', 'O...  \n1  ['O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', '...  \n2           ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']  \n3  ['B-ORG', 'I-ORG', 'I-ORG', 'O', 'B-ORG', 'O',...  \n4  ['O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', '...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tokens</th>\n      <th>tags</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>['\\ufeffউন্নয়নের', 'বিস্ময়', 'বাংলাদেশ', 'বাংল...</td>\n      <td>['O', 'O', 'B-LOC', 'B-LOC', 'O', 'O', 'O', 'O...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>['অর্থনীতি', 'ও', 'আর্থসামাজিক', 'বেশির', 'ভাগ...</td>\n      <td>['O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', '...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>['নিম্ন', 'আয়ের', 'দেশগুলোকে', 'ছাড়িয়েছে', 'তো...</td>\n      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>['আন্তর্জাতিক', 'মুদ্রা', 'তহবিল', '(', 'আইএমএ...</td>\n      <td>['B-ORG', 'I-ORG', 'I-ORG', 'O', 'B-ORG', 'O',...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>['সবাইকে', 'অন্তর্ভুক্ত', 'করে', 'প্রবৃদ্ধি', ...</td>\n      <td>['O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', '...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# taking dataframe columns into lists\ntokens = df[\"tokens\"].tolist()\ntags   = df[\"tags\"].tolist()\nprint(len(tokens),len(tags))","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:24:36.844262Z","iopub.execute_input":"2023-07-11T14:24:36.844865Z","iopub.status.idle":"2023-07-11T14:24:36.853126Z","shell.execute_reply.started":"2023-07-11T14:24:36.844830Z","shell.execute_reply":"2023-07-11T14:24:36.852168Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"9926 9926\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now, these tokens and tags are in form of strings (whole sentences), we need to extract individual tokens and tags from it. So, I am extracting tokens from these sentences some punctuation that was causing error in the tokenization process","metadata":{}},{"cell_type":"markdown","source":"### Converting multiclass tags to binary tags","metadata":{}},{"cell_type":"code","source":"# tags and tokens fixing\nnewTags = []\nfor idx in range(len(tags)):\n    demo = tags[idx]\n    # getting individual tokens\n    demo = demo.replace(\"[\", \"\" )# at first tabs necessary to add count\n    demo = demo.replace(\"]\", \"\" )# at first tabs necessary to add count\n    demo = demo.replace(\"'\", \"\" )# at first tabs necessary to add count\n    demo = demo.replace(\" \", \"\" )# at first tabs necessary to add count\n    demo = demo.split(\",\")\n    \n    \"\"\" The conversion takes place here \"\"\"\n    for j in range(len(demo)):\n        if demo[j] == \"B-PER\" or  demo[j] == \"I-PER\":\n            demo[j] = \"B-PER\"\n        else:\n            demo[j] = \"O\"\n\n    newTags.append(demo)\n    \ntags = newTags\n#############\nnewTokens = []\nfor idx in range(len(tokens)):\n    demo = tokens[idx]\n    # getting individual tokens\n    demo = demo.replace(\"[\", \"\" )# at first tabs necessary to add count\n    demo = demo.replace(\"]\", \"\" )# at first tabs necessary to add count\n    demo = demo.replace(\"'\", \"\" )# at first tabs necessary to add count\n    demo = demo.replace(\" \", \"\" )# at first tabs necessary to add count\n    demo = demo.split(\",\")\n\n    newTokens.append(demo)\n    \ntokens = newTokens\nprint(len(tokens),len(tags))","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:24:36.854955Z","iopub.execute_input":"2023-07-11T14:24:36.855332Z","iopub.status.idle":"2023-07-11T14:24:37.033768Z","shell.execute_reply.started":"2023-07-11T14:24:36.855305Z","shell.execute_reply":"2023-07-11T14:24:37.032651Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"9926 9926\n","output_type":"stream"}]},{"cell_type":"code","source":"# checking \nprint(tokens[0][0])\nprint(tags[0][0])","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:24:37.035306Z","iopub.execute_input":"2023-07-11T14:24:37.036325Z","iopub.status.idle":"2023-07-11T14:24:37.042108Z","shell.execute_reply.started":"2023-07-11T14:24:37.036290Z","shell.execute_reply":"2023-07-11T14:24:37.041148Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"\\ufeffউন্নয়নের\nO\n","output_type":"stream"}]},{"cell_type":"markdown","source":"now I am checking the unique label names for the encoding process coming next","metadata":{}},{"cell_type":"code","source":"flattened_list = list(itertools.chain(*tags))\nlabel_names = list(set(flattened_list))\nlabel_names# checking unique tag names","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:24:37.043388Z","iopub.execute_input":"2023-07-11T14:24:37.044285Z","iopub.status.idle":"2023-07-11T14:24:37.060935Z","shell.execute_reply.started":"2023-07-11T14:24:37.044248Z","shell.execute_reply":"2023-07-11T14:24:37.060011Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"['O', 'B-PER']"},"metadata":{}}]},{"cell_type":"markdown","source":"### converting tags to integers","metadata":{}},{"cell_type":"code","source":"tags[:3]","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:24:37.062433Z","iopub.execute_input":"2023-07-11T14:24:37.062864Z","iopub.status.idle":"2023-07-11T14:24:37.072638Z","shell.execute_reply.started":"2023-07-11T14:24:37.062832Z","shell.execute_reply":"2023-07-11T14:24:37.071957Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"[['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]"},"metadata":{}}]},{"cell_type":"code","source":"for i in range(len(tags)):\n    demo =tags[i]\n    for j in range(len(demo)):\n        demo[j] = label_names.index(demo[j])\n    tags[i] = demo\n\nfor j in range(3):\n    print(tags[i])","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:28:30.146077Z","iopub.execute_input":"2023-07-11T14:28:30.146818Z","iopub.status.idle":"2023-07-11T14:28:30.206316Z","shell.execute_reply.started":"2023-07-11T14:28:30.146785Z","shell.execute_reply":"2023-07-11T14:28:30.205334Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","output_type":"stream"}]},{"cell_type":"code","source":"# checking alignment of token and tags\nwords = tokens[0]\nlabels = tags[0]\nline1 = \"\"\nline2 = \"\"\nfor word, label in zip(words, labels):\n    full_label = label_names[label]\n    max_length = max(len(word), len(full_label))\n    line1 += word + \" \" * (max_length - len(word) + 1)\n    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n\nprint(line1)\nprint(line2)","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:28:30.208165Z","iopub.execute_input":"2023-07-11T14:28:30.209116Z","iopub.status.idle":"2023-07-11T14:28:30.218037Z","shell.execute_reply.started":"2023-07-11T14:28:30.209080Z","shell.execute_reply":"2023-07-11T14:28:30.217105Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"\\ufeffউন্নয়নের বিস্ময় বাংলাদেশ বাংলাদেশের অগ্রগতি উদাহরণ দেওয়ার মতোই । \nO              O      O        O          O       O      O      O    O \n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Tokenizing (for the model)\n\n* using pretrained tokenizer suitable for our model","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:28:30.219427Z","iopub.execute_input":"2023-07-11T14:28:30.219832Z","iopub.status.idle":"2023-07-11T14:28:31.941291Z","shell.execute_reply.started":"2023-07-11T14:28:30.219799Z","shell.execute_reply":"2023-07-11T14:28:31.940249Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/298 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1950adcdd315488b965adaebe1bef7cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.13k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10787704aa884984a3db48c257e11e4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/872k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6efb97da4574413d8fa4a6aee5491e8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.72M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d84b93f5a2ec4349994a586d88298c95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53a5a10b94714dcda2fc012c9dd1c981"}},"metadata":{}}]},{"cell_type":"code","source":"# checking tokenizer at play, it will add additional level for the understanding of the model\ninputs = tokenizer(tokens[0], is_split_into_words=True)\nprint(inputs.tokens())\nprint(inputs.word_ids()) # priniting word id associated with this","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:28:31.944003Z","iopub.execute_input":"2023-07-11T14:28:31.944462Z","iopub.status.idle":"2023-07-11T14:28:31.956441Z","shell.execute_reply.started":"2023-07-11T14:28:31.944424Z","shell.execute_reply":"2023-07-11T14:28:31.955232Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"['[CLS]', '\\\\', 'u', '##fe', '##ff', '##উ', '##ন', '##ন', '##যন', '##ের', 'ব', '##িস', '##ময', 'বাংলাদেশ', 'বাংলাদেশের', 'অ', '##গর', '##গত', '##ি', 'উ', '##দা', '##হ', '##রণ', 'দেওযা', '##র', 'মতো', '##ই', '।', '[SEP]']\n[None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 7, 7, 8, None]\n","output_type":"stream"}]},{"cell_type":"code","source":"def align_labels_with_tokens(labels, word_ids):\n    \"\"\"\n        This function aligns labels and word_ids\n    \"\"\"\n    new_labels = []\n    current_word = None\n    for word_id in word_ids:\n        if word_id != current_word:\n            # Start of a new word!\n            current_word = word_id\n            label = -100 if word_id is None else labels[word_id]\n            new_labels.append(label)\n        elif word_id is None:\n            # Special token\n            new_labels.append(-100)\n        else:\n            # Same word as previous token\n            label = labels[word_id]\n            # If the label is B-XXX we change it to I-XXX\n            if label % 2 == 1:\n                label += 1\n            new_labels.append(label)\n\n    return new_labels","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:28:31.958100Z","iopub.execute_input":"2023-07-11T14:28:31.958444Z","iopub.status.idle":"2023-07-11T14:28:31.965337Z","shell.execute_reply.started":"2023-07-11T14:28:31.958411Z","shell.execute_reply":"2023-07-11T14:28:31.964371Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# checking it\nlabels = tags[0]\nword_ids = inputs.word_ids()\nprint(labels)\nprint(align_labels_with_tokens(labels, word_ids))","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:28:31.967052Z","iopub.execute_input":"2023-07-11T14:28:31.967737Z","iopub.status.idle":"2023-07-11T14:28:31.976590Z","shell.execute_reply.started":"2023-07-11T14:28:31.967702Z","shell.execute_reply":"2023-07-11T14:28:31.975554Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"[0, 0, 0, 0, 0, 0, 0, 0, 0]\n[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Creating datasets","metadata":{}},{"cell_type":"code","source":"def cleaning_data(tokens, tags):\n    \"\"\"\n        This function deletes all the mismatched token-tag pairs and generate flags which will be used\n        for splitting the dataset\n    \"\"\"\n    mismatch = []\n    newtokens = []\n    newtags   = []\n    flags      = []\n    newcount = 0\n    for i in range(len(tokens)):\n        c = 0\n        if len(tokens[i]) > 0 and len(tags[i])>0:\n            if len(tokens[i]) != len(tags[i]):\n                mismatch.append(i)\n            else:\n                newtokens.append(tokens[i])\n                newtags.append(tags[i])\n                \n                demo = tags[i]\n                for j in demo:\n                    if j == 1 or j == 2:\n                        c = 1\n                if c == 1:\n                    flags.append(1)\n                else:\n                    flags.append(0)\n                \n\n                \n    print(\"total number of flag sentences %d\"%(sum(flags)))\n    print(len(flags),\"\\n\")\n    print(\"total number of mismatched token and tag pair sentneces %d\"%(len(mismatch)))\n    print(\"total number of sentneces %d\"%(i+1))\n    print(\"final length of dataset %d\"%len(newtokens))\n    return newtokens,newtags,flags\n\ntokens,tags,flags = cleaning_data(tokens, tags)","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:28:31.977907Z","iopub.execute_input":"2023-07-11T14:28:31.978403Z","iopub.status.idle":"2023-07-11T14:28:32.011249Z","shell.execute_reply.started":"2023-07-11T14:28:31.978365Z","shell.execute_reply":"2023-07-11T14:28:32.010124Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"total number of flag sentences 2104\n7963 \n\ntotal number of mismatched token and tag pair sentneces 1963\ntotal number of sentneces 9926\nfinal length of dataset 7963\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### splitting the dataset","metadata":{}},{"cell_type":"code","source":"# Split the data into train, validation, and test datasets\ntokens_train, tokens_val, tags_train, tags_val = train_test_split(tokens, tags,stratify=flags, test_size=0.2, random_state=42)\ntokens_val, tokens_test, tags_val, tags_test = train_test_split(tokens_val, tags_val, test_size=0.25, random_state=42)\n\n# Print the split datasets\nprint(\"Train dataset:\")\nprint(tokens_train[0])\nprint(tags_train[0])\nprint(len(tokens_train),\"\\n\")\n\nprint(\"Validation dataset:\")\nprint(tokens_val[0])\nprint(tags_val[0])\nprint(len(tokens_val),\"\\n\")\n\nprint(\"Test dataset:\")\nprint(tokens_test[0])\nprint(tags_test[0])\nprint(len(tokens_test),\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:28:32.013144Z","iopub.execute_input":"2023-07-11T14:28:32.013833Z","iopub.status.idle":"2023-07-11T14:28:32.035064Z","shell.execute_reply.started":"2023-07-11T14:28:32.013799Z","shell.execute_reply":"2023-07-11T14:28:32.034218Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Train dataset:\n['বিবিসির', 'প্রতিবেদনে', 'বলা', 'হয়েছে', '<PUNCT>', '২০১৪', 'সালের', 'শুমারি', 'অনুযায়ী', 'ভারতে', 'বাঘের', 'সংখ্যা', 'এখন', '২', 'হাজার', '২২৬টি', '<PUNCT>']\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n6370 \n\nValidation dataset:\n['যুক্তরাষ্ট্রের', 'একজন', 'শীর্ষ', 'কর্মকর্তা', 'গতকাল', 'বুধবার', 'এ', 'তথ্য', 'জানান', '।']\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n1194 \n\nTest dataset:\n['ফাতেমা', 'বেগম', 'জানান', '<PUNCT>', 'তাঁর', 'স্বামী', 'গণি', 'এক', 'বছর', 'ধরে', 'এক', 'নারীর', 'সঙ্গে', 'পরকীয়ায়', 'জড়িয়ে', 'পড়েন', '<PUNCT>']\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n399 \n\n","output_type":"stream"}]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:28:32.036594Z","iopub.execute_input":"2023-07-11T14:28:32.037313Z","iopub.status.idle":"2023-07-11T14:28:32.501441Z","shell.execute_reply.started":"2023-07-11T14:28:32.037276Z","shell.execute_reply":"2023-07-11T14:28:32.500289Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"314"},"metadata":{}}]},{"cell_type":"markdown","source":"## preparing the dataset","metadata":{}},{"cell_type":"code","source":"# Create Dataset objects for train, validation, and test splits\ntrain_dataset = Dataset.from_dict({'tokens': tokens_train, 'tags': tags_train})\nvalidation_dataset = Dataset.from_dict({'tokens': tokens_val, 'tags': tags_val})\ntest_dataset = Dataset.from_dict({'tokens': tokens_test, 'tags': tags_test})\n\n# Create a DatasetDict object\nraw_datasets = DatasetDict({\n    'train': train_dataset,\n    'validation': validation_dataset,\n    'test': test_dataset\n})","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:28:32.506807Z","iopub.execute_input":"2023-07-11T14:28:32.507528Z","iopub.status.idle":"2023-07-11T14:28:32.636048Z","shell.execute_reply.started":"2023-07-11T14:28:32.507494Z","shell.execute_reply":"2023-07-11T14:28:32.635029Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"raw_datasets[\"train\"].column_names","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:28:32.637824Z","iopub.execute_input":"2023-07-11T14:28:32.638514Z","iopub.status.idle":"2023-07-11T14:28:32.649615Z","shell.execute_reply.started":"2023-07-11T14:28:32.638478Z","shell.execute_reply":"2023-07-11T14:28:32.648656Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"['tokens', 'tags']"},"metadata":{}}]},{"cell_type":"code","source":"def tokenize_and_align_labels(examples):\n    \"\"\"\n        this function will tokenize using pretrained tokenizer and align labels and word ids\n    \"\"\"\n    tokenized_inputs = tokenizer(\n        examples[\"tokens\"], truncation=True, is_split_into_words=True\n    )\n    all_labels = examples[\"tags\"]\n    new_labels = []\n    for i, labels in enumerate(all_labels):\n        word_ids = tokenized_inputs.word_ids(i)\n        new_labels.append(align_labels_with_tokens(labels, word_ids))\n\n    tokenized_inputs[\"labels\"] = new_labels\n    return tokenized_inputs","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:28:32.651301Z","iopub.execute_input":"2023-07-11T14:28:32.652075Z","iopub.status.idle":"2023-07-11T14:28:32.663743Z","shell.execute_reply.started":"2023-07-11T14:28:32.652039Z","shell.execute_reply":"2023-07-11T14:28:32.662898Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# mapping the datasets\ntokenized_datasets = raw_datasets.map(\n    tokenize_and_align_labels,\n    batched=True,\n    remove_columns=raw_datasets[\"train\"].column_names,\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:28:32.666938Z","iopub.execute_input":"2023-07-11T14:28:32.667910Z","iopub.status.idle":"2023-07-11T14:28:35.228253Z","shell.execute_reply.started":"2023-07-11T14:28:32.667869Z","shell.execute_reply":"2023-07-11T14:28:35.227249Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/7 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ede19ec92e34bcd885d39239934ab8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddd7c4a416b44c34bc81c94ecba69498"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c52b37be71304200bbdd1ae2ef5d34a0"}},"metadata":{}}]},{"cell_type":"code","source":"# collat function for preparing dataset\ndata_collator = DataCollatorForTokenClassification(\n    tokenizer=tokenizer, return_tensors=\"tf\"\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:28:35.229935Z","iopub.execute_input":"2023-07-11T14:28:35.230659Z","iopub.status.idle":"2023-07-11T14:28:35.235586Z","shell.execute_reply.started":"2023-07-11T14:28:35.230620Z","shell.execute_reply":"2023-07-11T14:28:35.234691Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# checking on a batch\nbatch = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\nbatch[\"labels\"]","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:28:35.237390Z","iopub.execute_input":"2023-07-11T14:28:35.237826Z","iopub.status.idle":"2023-07-11T14:28:40.810989Z","shell.execute_reply.started":"2023-07-11T14:28:35.237795Z","shell.execute_reply":"2023-07-11T14:28:40.810046Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(2, 43), dtype=int64, numpy=\narray([[-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0, -100, -100, -100],\n       [-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0, -100]])>"},"metadata":{}}]},{"cell_type":"code","source":"# finalizing the datasets\ntf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n    columns=[\"attention_mask\", \"input_ids\", \"labels\", \"token_type_ids\"],\n    collate_fn=data_collator,\n    shuffle=True,\n    batch_size=8,#16\n)\n\ntf_eval_dataset = tokenized_datasets[\"validation\"].to_tf_dataset(\n    columns=[\"attention_mask\", \"input_ids\", \"labels\", \"token_type_ids\"],\n    collate_fn=data_collator,\n    shuffle=False,\n    batch_size=8,#16\n)\n\ntf_test_dataset = tokenized_datasets[\"validation\"].to_tf_dataset(\n    columns=[\"attention_mask\", \"input_ids\", \"labels\", \"token_type_ids\"],\n    collate_fn=data_collator,\n    shuffle=False,\n    batch_size=8,#16\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:28:40.812274Z","iopub.execute_input":"2023-07-11T14:28:40.812635Z","iopub.status.idle":"2023-07-11T14:28:41.062391Z","shell.execute_reply.started":"2023-07-11T14:28:40.812599Z","shell.execute_reply":"2023-07-11T14:28:41.061387Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"prevnames = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:28:41.063790Z","iopub.execute_input":"2023-07-11T14:28:41.064425Z","iopub.status.idle":"2023-07-11T14:28:41.069895Z","shell.execute_reply.started":"2023-07-11T14:28:41.064389Z","shell.execute_reply":"2023-07-11T14:28:41.068706Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# creating mapping dictionary for ease of mapping\nid2label = {i: label for i, label in enumerate(prevnames)}\nlabel2id = {v: k for k, v in id2label.items()}","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:28:41.071410Z","iopub.execute_input":"2023-07-11T14:28:41.072502Z","iopub.status.idle":"2023-07-11T14:28:41.080355Z","shell.execute_reply.started":"2023-07-11T14:28:41.072466Z","shell.execute_reply":"2023-07-11T14:28:41.079365Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:28:41.081837Z","iopub.execute_input":"2023-07-11T14:28:41.082305Z","iopub.status.idle":"2023-07-11T14:28:41.431830Z","shell.execute_reply.started":"2023-07-11T14:28:41.082270Z","shell.execute_reply":"2023-07-11T14:28:41.430621Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"46"},"metadata":{}}]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"markdown","source":"## Building the model","metadata":{}},{"cell_type":"code","source":"# Initialize the model\nmodel = TFAutoModelForTokenClassification.from_pretrained(\n    model_checkpoint,\n    id2label=id2label,\n    label2id=label2id,\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:37:21.558985Z","iopub.execute_input":"2023-07-11T14:37:21.559706Z","iopub.status.idle":"2023-07-11T14:37:24.510695Z","shell.execute_reply.started":"2023-07-11T14:37:21.559670Z","shell.execute_reply":"2023-07-11T14:37:24.509704Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForTokenClassification: ['bert.embeddings.position_ids']\n- This IS expected if you are initializing TFBertForTokenClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertForTokenClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertForTokenClassification were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"code","source":"# checking number of classes with the model\nmodel.config.num_labels","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:37:24.512940Z","iopub.execute_input":"2023-07-11T14:37:24.513346Z","iopub.status.idle":"2023-07-11T14:37:24.519874Z","shell.execute_reply.started":"2023-07-11T14:37:24.513310Z","shell.execute_reply":"2023-07-11T14:37:24.518936Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"7"},"metadata":{}}]},{"cell_type":"markdown","source":"## compiling the model","metadata":{}},{"cell_type":"code","source":"# mixed precision (mainly needed if ran in CPU)\ntf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n\nnum_train_steps = len(tf_train_dataset) * num_epochs\n\noptimizer, schedule = create_optimizer(\n    init_lr=INIT_LR,\n    num_warmup_steps=0,\n    num_train_steps=num_train_steps,\n    weight_decay_rate=WD,\n)\nmodel.compile(optimizer=optimizer)","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:37:24.522014Z","iopub.execute_input":"2023-07-11T14:37:24.522751Z","iopub.status.idle":"2023-07-11T14:37:24.548278Z","shell.execute_reply.started":"2023-07-11T14:37:24.522714Z","shell.execute_reply":"2023-07-11T14:37:24.547327Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"## Training the model","metadata":{}},{"cell_type":"code","source":"callback = PushToHubCallback(output_dir=dirname, tokenizer=tokenizer)\n\nmodel.fit(\n    tf_train_dataset,\n    validation_data=tf_eval_dataset,\n    callbacks=[callback],\n    epochs=num_epochs,\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:37:24.551086Z","iopub.execute_input":"2023-07-11T14:37:24.551784Z","iopub.status.idle":"2023-07-11T14:44:25.513193Z","shell.execute_reply.started":"2023-07-11T14:37:24.551750Z","shell.execute_reply":"2023-07-11T14:44:25.512038Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stderr","text":"/kaggle/working/mbert-ner-baseline-1 is already a clone of https://huggingface.co/imtiaz114/mbert-ner-baseline-1. Make sure you pull the latest changes with `repo.git_pull()`.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/2\n796/796 [==============================] - 175s 191ms/step - loss: 0.1014 - val_loss: 0.0652\nEpoch 2/2\n796/796 [==============================] - 173s 217ms/step - loss: 0.0533 - val_loss: 0.0578\n","output_type":"stream"},{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7d863d004220>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"metric = evaluate.load(\"seqeval\")","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:44:25.514940Z","iopub.execute_input":"2023-07-11T14:44:25.515403Z","iopub.status.idle":"2023-07-11T14:44:26.109385Z","shell.execute_reply.started":"2023-07-11T14:44:25.515355Z","shell.execute_reply":"2023-07-11T14:44:26.108459Z"},"trusted":true},"execution_count":42,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1139d41798a041b8ad92dd7e29500a19"}},"metadata":{}}]},{"cell_type":"code","source":"all_predictions = []\nall_labels = []\nfor batch in tf_test_dataset:\n    logits = model.predict_on_batch(batch)[\"logits\"]\n    labels = batch[\"labels\"]\n    predictions = np.argmax(logits, axis=-1)\n    for prediction, label in zip(predictions, labels):\n        for predicted_idx, label_idx in zip(prediction, label):\n            if label_idx == -100:\n                continue\n            all_predictions.append(prevnames[predicted_idx])\n            all_labels.append(prevnames[label_idx])\n# computing classification metrics\nmetric.compute(predictions=[all_predictions], references=[all_labels])","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:45:39.847388Z","iopub.execute_input":"2023-07-11T14:45:39.847797Z","iopub.status.idle":"2023-07-11T14:47:11.072229Z","shell.execute_reply.started":"2023-07-11T14:45:39.847765Z","shell.execute_reply":"2023-07-11T14:47:11.071168Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"{'PER': {'precision': 0.68375,\n  'recall': 0.7048969072164949,\n  'f1': 0.6941624365482233,\n  'number': 776},\n 'overall_precision': 0.68375,\n 'overall_recall': 0.7048969072164949,\n 'overall_f1': 0.6941624365482233,\n 'overall_accuracy': 0.9747065399664617}"},"metadata":{}}]},{"cell_type":"markdown","source":"# Prediction (on sentences)","metadata":{}},{"cell_type":"code","source":"model_checkpoint = dirname\ntoken_classifier = pipeline(\n    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n)\n\ntext = \"প্রতিবেদক ছিলেন সাপ্তাহিকের রাজশাহীর বিভাগীয় প্রতিনিধি সায়েম সাবু ।\"\ntoken_classifier(text)","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:48:56.321965Z","iopub.execute_input":"2023-07-11T14:48:56.322660Z","iopub.status.idle":"2023-07-11T14:49:11.341655Z","shell.execute_reply.started":"2023-07-11T14:48:56.322622Z","shell.execute_reply":"2023-07-11T14:49:11.340675Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stderr","text":"Some layers from the model checkpoint at /kaggle/working/mbert-ner-baseline-1 were not used when initializing TFBertForTokenClassification: ['dropout_75']\n- This IS expected if you are initializing TFBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertForTokenClassification were initialized from the model checkpoint at /kaggle/working/mbert-ner-baseline-1.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n","output_type":"stream"},{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"[{'entity_group': 'PER',\n  'score': 0.989,\n  'word': 'সাযেম',\n  'start': 55,\n  'end': 60},\n {'entity_group': 'PER',\n  'score': 0.988,\n  'word': 'সাব',\n  'start': 61,\n  'end': 64}]"},"metadata":{}}]},{"cell_type":"code","source":"def gettingPersonName(inputs):\n    \"\"\"\n        This function takes inputs a sentence and gives persons name output\n    \"\"\"\n    print(inputs)\n    output = token_classifier(inputs)\n    \n    \n    person_names = []\n    current_person = \"\"\n\n    for entity in output:\n        if entity['entity_group'] == 'PER':\n            current_person += entity['word']\n            person_names.append(current_person)\n            current_person = \"\"\n\n        else:\n            if current_person:\n                #person_names.append(current_person)\n                current_person = \"\"\n\n    # Add the last person name entity if present\n    if current_person:\n        person_names.append(current_person)\n        \n    clean_names=[]\n    for demo in person_names:\n        demo = demo.replace(\"#\", \"\" )\n        clean_names.append(demo)\n    \n    if len(clean_names)>0:\n        for names in clean_names:\n            print(names)\n    else:\n        print(\"There is no person in this sentence\")\n        \n    return clean_names\n\ntext = \"প্রতিবেদক ছিলেন সাপ্তাহিকের রাজশাহীর বিভাগীয় প্রতিনিধি সায়েম সাবু ।\"\nnames = gettingPersonName(text)","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:49:11.343861Z","iopub.execute_input":"2023-07-11T14:49:11.344255Z","iopub.status.idle":"2023-07-11T14:49:23.883640Z","shell.execute_reply.started":"2023-07-11T14:49:11.344221Z","shell.execute_reply":"2023-07-11T14:49:23.882661Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"প্রতিবেদক ছিলেন সাপ্তাহিকের রাজশাহীর বিভাগীয় প্রতিনিধি সায়েম সাবু ।\nসাযেম\nসাব\n","output_type":"stream"}]},{"cell_type":"code","source":"text =  \"নিরাপত্তা কর্মকর্তা জান মোহাম্মদ আদালতকে মোশাররফের ‘অসুস্থতা’ সম্পর্কে অবহিত করেন ।\"\nnames = gettingPersonName(text)","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:49:23.886166Z","iopub.execute_input":"2023-07-11T14:49:23.886811Z","iopub.status.idle":"2023-07-11T14:49:41.485467Z","shell.execute_reply.started":"2023-07-11T14:49:23.886780Z","shell.execute_reply":"2023-07-11T14:49:41.483282Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"নিরাপত্তা কর্মকর্তা জান মোহাম্মদ আদালতকে মোশাররফের ‘অসুস্থতা’ সম্পর্কে অবহিত করেন ।\nজান\nমোহামমদ\nআ\nকে\nমোশাররফের\n","output_type":"stream"}]},{"cell_type":"code","source":"text = \"ক্রমবর্ধমান রাজনৈতিক উত্তেজনার সঙ্গে মানবতাবিরোধী অপরাধে বিচারের বিষয়টি যুক্ত হয়ে সারা দেশে নিরাপত্তা পরিস্থিতির অবনতি হচ্ছে ।\"\nnames = gettingPersonName(text)","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:49:41.487809Z","iopub.execute_input":"2023-07-11T14:49:41.488446Z","iopub.status.idle":"2023-07-11T14:50:06.553776Z","shell.execute_reply.started":"2023-07-11T14:49:41.488407Z","shell.execute_reply":"2023-07-11T14:50:06.552679Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"ক্রমবর্ধমান রাজনৈতিক উত্তেজনার সঙ্গে মানবতাবিরোধী অপরাধে বিচারের বিষয়টি যুক্ত হয়ে সারা দেশে নিরাপত্তা পরিস্থিতির অবনতি হচ্ছে ।\nThere is no person in this sentence\n","output_type":"stream"}]},{"cell_type":"code","source":"text = \"প্রেমের ময়দানে ক্যারোলিন ওজনিয়াকি আর ররি ম্যাকইলরয়ের লক্ষণীয় উন্নতিই হয়েছে ।\"\nnames = gettingPersonName(text)","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:50:06.555075Z","iopub.execute_input":"2023-07-11T14:50:06.555425Z","iopub.status.idle":"2023-07-11T14:50:21.182051Z","shell.execute_reply.started":"2023-07-11T14:50:06.555390Z","shell.execute_reply":"2023-07-11T14:50:21.181059Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"প্রেমের ময়দানে ক্যারোলিন ওজনিয়াকি আর ররি ম্যাকইলরয়ের লক্ষণীয় উন্নতিই হয়েছে ।\nকযারোলিন\nওজনিযাকি\nররি\nমযাকইলরযের\n","output_type":"stream"}]},{"cell_type":"code","source":"text = \"আমার সোনার বাংলা আমি তোমায় ভালবাসি\"\nnames = gettingPersonName(text)","metadata":{"execution":{"iopub.status.busy":"2023-07-11T14:50:21.183351Z","iopub.execute_input":"2023-07-11T14:50:21.183729Z","iopub.status.idle":"2023-07-11T14:50:28.765325Z","shell.execute_reply.started":"2023-07-11T14:50:21.183692Z","shell.execute_reply":"2023-07-11T14:50:28.764214Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"আমার সোনার বাংলা আমি তোমায় ভালবাসি\nThere is no person in this sentence\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Conclusion","metadata":{}}]}