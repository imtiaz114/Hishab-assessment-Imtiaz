1. Bangla bert base

{'PER': {'precision': 0.7379310344827587,
  'recall': 0.7086092715231788,
  'f1': 0.722972972972973,
  'number': 302},
 'overall_precision': 0.7379310344827587,
 'overall_recall': 0.7086092715231788,
 'overall_f1': 0.722972972972973,
 'overall_accuracy': 0.9803281433115687}


2. indic-bert

{'PER': {'precision': 0.6440677966101694,
  'recall': 0.5033112582781457,
  'f1': 0.5650557620817844,
  'number': 302},
 'overall_precision': 0.6440677966101694,
 'overall_recall': 0.5033112582781457,
 'overall_f1': 0.5650557620817844,
 'overall_accuracy': 0.9684125269978402}

3. m-bert

{'PER': {'precision': 0.7642512077294686,
  'recall': 0.8231009365244537,
  'f1': 0.7925851703406814,
  'number': 2883},
 'overall_precision': 0.7642512077294686,
 'overall_recall': 0.8231009365244537,
 'overall_f1': 0.7925851703406814,
 'overall_accuracy': 0.9831450288709345}

4. xlm-roberta-base

{'PER': {'precision': 0.6908171861836563,
  'recall': 0.7606679035250464,
  'f1': 0.7240618101545254,
  'number': 1078},
 'overall_precision': 0.6908171861836563,
 'overall_recall': 0.7606679035250464,
 'overall_f1': 0.7240618101545254,
 'overall_accuracy': 0.9737357424043606}

5. custom model

processed 5148 tokens with 722 phrases; found: 771 phrases; correct: 584.
accuracy:  79.58%; (non-O)
accuracy:  93.53%; precision:  75.75%; recall:  80.89%; FB1:  78.23
              LOC: precision:  73.08%; recall:  73.64%; FB1:  73.36  130
              ORG: precision:  66.48%; recall:  65.73%; FB1:  66.10  176
              PER: precision:  80.00%; recall:  89.64%; FB1:  84.55  465
5099 predicted tokens
Counter({'O': 4213, 'B-PER': 317, 'I-PER': 259, 'B-LOC': 130, 'I-ORG': 125, 'B-ORG': 103, 'I-LOC': 1})
real tokens
Counter({'O': 4237, 'B-PER': 283, 'I-PER': 259, 'I-ORG': 125, 'B-LOC': 115, 'B-ORG': 110, 'I-LOC': 19})
Accuracy: 93.53%

Among these, my custom model has the higest precision and recall in terms of "PER" entity.

4. 